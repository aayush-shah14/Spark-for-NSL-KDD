{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fd0344",
   "metadata": {},
   "source": [
    "# This Notebook should be run on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c167db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if you are using Windows!\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "#findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"SparkMLOct10-RF500-yarn\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!python -m wget https://www.andrew.cmu.edu/user/mfarag/14763/KDDTrain+.txt\n",
    "!python -m wget https://www.andrew.cmu.edu/user/mfarag/14763/KDDTest+.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c47cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put KDDTrain+.txt /\n",
    "!hadoop fs -put KDDTest+.txt /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"class\",\"difficulty\"]\n",
    "\n",
    "nominal_cols = ['protocol_type','service','flag']\n",
    "binary_cols = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login',\n",
    "'is_guest_login']\n",
    "continuous_cols = ['duration' ,'src_bytes', 'dst_bytes', 'wrong_fragment' ,'urgent', 'hot',\n",
    "'num_failed_logins', 'num_compromised', 'num_root' ,'num_file_creations',\n",
    "'num_shells', 'num_access_files', 'num_outbound_cmds', 'count' ,'srv_count',\n",
    "'serror_rate', 'srv_serror_rate' ,'rerror_rate' ,'srv_rerror_rate',\n",
    "'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate' ,'dst_host_count',\n",
    "'dst_host_srv_count' ,'dst_host_same_srv_rate' ,'dst_host_diff_srv_rate',\n",
    "'dst_host_same_src_port_rate' ,'dst_host_srv_diff_host_rate',\n",
    "'dst_host_serror_rate' ,'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate']\n",
    "\n",
    "def class_to_type(name): #class which returns encoded class types based on name\n",
    "    if name == 'normal':\n",
    "        return 0\n",
    "    elif name in ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', 'processtable', 'smurf', 'teardrop', 'udpstorm', 'worm']:\n",
    "        return 1 # for DOS attacks\n",
    "    elif name in ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan']:\n",
    "        return 2 # for Probe attacks\n",
    "    elif name in ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm']:\n",
    "        return 3 # for U2R\n",
    "    elif name in ['ftp_write', 'guess_passwd', 'httptunnel', 'imap', 'multihop', 'named', 'phf', 'sendmail', 'Snmpgetattack', 'spy', 'snmpguess', 'warezclient', 'warezmaster', 'xlock', 'xsnoop']:\n",
    "        return 4 # for R2L\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        label_to_binary = udf(lambda name: class_to_type(name))\n",
    "        output_df = dataset.withColumn('outcome', label_to_binary(col('class'))).drop(\"class\")  \n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        output_df = output_df.drop('difficulty')\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "    corelated_cols_to_remove = [\"dst_host_serror_rate\",\"srv_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "                     \"srv_rerror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n",
    "    for col_name in corelated_cols_to_remove:\n",
    "        feature_cols.remove(col_name)\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n",
    "        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n",
    "    return pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nslkdd_raw = spark.read.csv('./NSL-KDD/KDDTrain+.txt',header=False).toDF(*col_names)\n",
    "nslkdd_test_raw = spark.read.csv('./NSL-KDD/KDDTest+.txt',header=False).toDF(*col_names)\n",
    "\n",
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(nslkdd_raw)\n",
    "\n",
    "nslkdd_df = preprocess_pipeline_model.transform(nslkdd_raw)\n",
    "nslkdd_df_test = preprocess_pipeline_model.transform(nslkdd_test_raw)\n",
    "\n",
    "nslkdd_df.cache()\n",
    "nslkdd_df_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821df6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "lrModel = lr.fit(nslkdd_df) # fit the logistic regression model to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr = lrModel.transform(nslkdd_df_test)\n",
    "predictions_lr_train = lrModel.transform(nslkdd_df)\n",
    "\n",
    "accuracy_train_lr = (predictions_lr_train.filter(predictions_lr_train.outcome == predictions_lr_train.prediction)\n",
    "    .count() / float(predictions_lr_train.count()))\n",
    "\n",
    "accuracy_test_lr = (predictions_lr.filter(predictions_lr.outcome == predictions_lr.prediction)\n",
    "    .count() / float(predictions_lr.count()))\n",
    "print(f\"Train Accuracy for Logistic Regression : {np.round(accuracy_train_lr*100,2)}%\")\n",
    "print(f\"Test Accuracy for Logistic Regression: {np.round(accuracy_test_lr*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c61da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
